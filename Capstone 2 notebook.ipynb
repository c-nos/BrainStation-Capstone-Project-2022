{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1629be01",
   "metadata": {},
   "source": [
    "# Capstone Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b28fe5",
   "metadata": {},
   "source": [
    "## Casey Nosiglia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9278b10",
   "metadata": {},
   "source": [
    "In this notebook, we will want to classify particular diseases among the `Tomato` species, which includes 10 different classes (1 healthy + 9 diseased). We will use the framework for creating a directory developed in the first notebook through the use of the dataframe `full_dataframe`, and then we will construct CNN models in order to try and learn the classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad546c78",
   "metadata": {},
   "source": [
    "## Load in relevant libraries (may need more later, but this should be sufficient for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all of the relevant libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image handling and path libraries\n",
    "import os\n",
    "import PIL\n",
    "import pathlib\n",
    "from PIL import Image \n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a62f75",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7057e65",
   "metadata": {},
   "source": [
    "In this section, we will create the image directory for the `Tomato` species, including a train/validation/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6072da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the full_dataframe to make more directory structures for CNN modelling:\n",
    "full_dataframe = pd.read_csv('full_dataframe.csv')\n",
    "full_dataframe.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
    "\n",
    "full_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b746a3ed",
   "metadata": {},
   "source": [
    "We want to only look at the class distribution for the `Tomato` species:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139ce3d",
   "metadata": {},
   "source": [
    "Tomato classes Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef7c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(full_dataframe[full_dataframe['Species']=='Tomato']['Disease_Type'].value_counts().index,full_dataframe[full_dataframe['Species']=='Tomato']['Disease_Type'].value_counts())\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution (Tomato)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbdffe7",
   "metadata": {},
   "source": [
    "We see that the class distribution for the `Tomato` species is highly imblanced, so we want to make an evenly distributed sample by upsampling and downsampling. Let's look at the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb71522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of the tomato diseases:\n",
    "full_dataframe[full_dataframe['Species'] == 'Tomato']['Disease_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df458",
   "metadata": {},
   "source": [
    "Based on the above numbers, we want to resample to 1000 in each category for hte `Tomato` species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c81a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices for the various tomato diseases:\n",
    "spec_dis_list = full_dataframe[full_dataframe['Species'] == 'Tomato']\\\n",
    "                ['Disease_Type'].value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_tomato = pd.DataFrame(columns = ['ID', 'is_diseased', 'Species','Disease_Type'])\n",
    "\n",
    "#resampling loop (including randomized downsampling and upsampling):\n",
    "for i in range(0,len(spec_dis_list)):\n",
    "    # downsampling\n",
    "    if full_dataframe.loc[(full_dataframe['Species'] == 'Tomato') & \n",
    "       (full_dataframe['Disease_Type'] == spec_dis_list[i])].shape[0] > 1000:\n",
    "        \n",
    "        resampled_tomato = pd.concat([resampled_tomato,\n",
    "                           full_dataframe.loc[(full_dataframe['Species'] == 'Tomato') &\n",
    "                          (full_dataframe['Disease_Type'] == spec_dis_list[i])].sample(1000)],axis = 0)\n",
    "    # upsampling    \n",
    "    else: \n",
    "        resampled_tomato = pd.concat([resampled_tomato,\n",
    "                    full_dataframe.loc[(full_dataframe['Species'] == 'Tomato') &\n",
    "                    (full_dataframe['Disease_Type'] == spec_dis_list[i])].sample(1000, replace = True)],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e74bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled_tomato.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_tomato.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex:\n",
    "resampled_tomato.reset_index(drop=True, inplace=True)\n",
    "resampled_tomato.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b681ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_tomato['Disease_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2d0af",
   "metadata": {},
   "source": [
    "We can make a plot of the Tomato class distribution after resampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d96dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(resampled_tomato[resampled_tomato['Species']=='Tomato']['Disease_Type'].value_counts().index,resampled_tomato[resampled_tomato['Species']=='Tomato']['Disease_Type'].value_counts())\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Class Distribution After Balancing (Tomato)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0e4ea",
   "metadata": {},
   "source": [
    "Now we want to create another directory of only the `Tomato` species using the `resampled_tomato` dataframe, with a train/validation/test structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the desired file paths:\n",
    "folders = ['Tomato___'+spec_dis_list[i] for i\\\n",
    "              in range(0,len(spec_dis_list))]\n",
    "\n",
    "# make empty directory structure:\n",
    "! mkdir resampled_tomato\n",
    "! mkdir resampled_tomato/test\n",
    "! mkdir resampled_tomato/train\n",
    "! mkdir resampled_tomato/validation\n",
    "for folder in folders:\n",
    "    os.mkdir(os.path.join('resampled_tomato/test/',folder))\n",
    "    os.mkdir(os.path.join('resampled_tomato/train/',folder))\n",
    "    os.mkdir(os.path.join('resampled_tomato/validation/',folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b060b809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the folders column and concatenate to the resampled_tomato df \n",
    "# for ease of manipulation:\n",
    "folders_col = pd.DataFrame({'Folders': \n",
    "              ['Tomato___'+ resampled_tomato['Disease_Type'][i] \n",
    "               for i in range(0,resampled_tomato['Disease_Type'].shape[0])]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad4374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled_tomato = pd.concat([resampled_tomato, folders_col], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a80a4",
   "metadata": {},
   "source": [
    "Now we want to fill the directory. We will need to make the train/validation/test split to iterate through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 train/test split:\n",
    "train_tomato, test_tomato = train_test_split(resampled_tomato,\n",
    "                                             test_size = 0.2,\n",
    "                                             stratify = resampled_tomato['Folders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8450319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 train/validation split:\n",
    "train_r_tomato, val_tomato = train_test_split(train_tomato,\n",
    "                                              test_size = 0.3,\n",
    "                                              stratify = train_tomato['Folders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681104ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_r_tomato['Folders'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722742e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tomato['Folders'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62311a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_tomato['Folders'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d21ac9",
   "metadata": {},
   "source": [
    "Now that we have the train/validation/test split, we want to fill up the tomato directory. Since we did upsampling and therefore will have some duplicates, we need the safe_copy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4580167",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''safe_copy function, which allows copying the same file path from a given directory\n",
    "   multiple times, by assigning a new file extension to the repeat copied file.'''\n",
    "# function taken from:\n",
    "# https://stackoverflow.com/questions/33282647/python-shutil-copy-if-i-have-a-duplicate-file-will-it-copy-to-new-location\n",
    "\n",
    "def safe_copy(file_path, out_dir, dst = None):\n",
    "    \"\"\"Safely copy a file to the specified directory. If a file with the same name already \n",
    "    exists, the copied file name is altered to preserve both.\n",
    "\n",
    "    :param str file_path: Path to the file to copy.\n",
    "    :param str out_dir: Directory to copy the file into.\n",
    "    :param str dst: New name for the copied file. If None, use the name of the original\n",
    "        file.\n",
    "    \"\"\"\n",
    "    name = dst or os.path.basename(file_path)\n",
    "    if not os.path.exists(os.path.join(out_dir, name)):\n",
    "        shutil.copy(file_path, os.path.join(out_dir, name))\n",
    "    else:\n",
    "        base, extension = os.path.splitext(name)\n",
    "        i = 1\n",
    "        while os.path.exists(os.path.join(out_dir, '{}_{}{}'.format(base, i, extension))):\n",
    "            i += 1\n",
    "        shutil.copy(file_path, os.path.join(out_dir, '{}_{}{}'.format(base, i, extension)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecddca",
   "metadata": {},
   "source": [
    "Now to fill up the `tomato` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54aa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IDs = train_r_tomato['ID'].to_list()\n",
    "test_IDs = test_tomato['ID'].to_list()\n",
    "val_IDs = val_tomato['ID'].to_list()\n",
    "\n",
    "for ID in train_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled_tomato/train/' + folder)\n",
    "            \n",
    "for ID in test_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled_tomato/test/' + folder)\n",
    "\n",
    "for ID in val_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled_tomato/validation/' + folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad515d",
   "metadata": {},
   "source": [
    "Now we have the directory `resampled_tomato` filled. Now we can load in the data using hte `ImageDataGenerator` functionality of Keras in order to generate the train, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a205c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 256\n",
    "width = 256\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "# We introduce data augmentation to the train set\n",
    "# in order to reduce overfitting.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range = 30,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled_tomato/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled_tomato/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# set the batch size to the number of images in the test directory:\n",
    "test_generator=test_datagen.flow_from_directory('resampled_tomato/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=2000,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321bd52",
   "metadata": {},
   "source": [
    "Let's look at a sample of the various classes for the `Tomato` species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c24d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of images from our validation generator: \n",
    "x, y = validation_generator.next() \n",
    "\n",
    "# Our images labels are one-hot encoded, lets \n",
    "# convert them to ordinal encoding\n",
    "labels = np.argmax(y, axis=1)\n",
    "\n",
    "label_names =  ['Tomato___Bacterial_spot',\n",
    "                'Tomato___Early_blight',\n",
    "                'Tomato___Late_blight',\n",
    "                'Tomato___Leaf_Mold',\n",
    "                'Tomato___Septoria_leaf_spot',\n",
    "                'Tomato___Spider_mites Two-spotted_spider_mite',\n",
    "                'Tomato___Target_Spot',\n",
    "                'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
    "                'Tomato___Tomato_mosaic_virus',\n",
    "                'Tomato___healthy']\n",
    "\n",
    "disease_names = [label_names[i].split(\"___\")[1] for i in range(len(label_names))]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3, figsize=(9, 14))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    index = np.argmax(labels==i) \n",
    "    ax[i].imshow(x[index])\n",
    "    \n",
    "    # Set the title of the subplot\n",
    "    ax[i].set_title(disease_names[i])\n",
    "    \n",
    "    # Hide the x and y ticks\n",
    "    ax[i].set_xticks([]) \n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "\n",
    "fig.suptitle(\"Classification Labels: Tomato\", size = 20)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Delete the two unused subplots\n",
    "fig.delaxes(ax[-1])\n",
    "fig.delaxes(ax[-2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892716dd",
   "metadata": {},
   "source": [
    "Looking at the above plots, we see that the diseases can cause wilting (such as for the `Tomato_Yellow_Leaf_Curl_Virus`), total discoloration (such as for the `Septoria_leaf_spot`), and also more subtle patterns. So we expect that our model will have to detect edge changes, as well as discoloration. We keep these expectations in mind when going into the modelling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2441ce0",
   "metadata": {},
   "source": [
    "### First try at a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f4bbb",
   "metadata": {},
   "source": [
    "But first, we can try a a simple model as we did in the Diseased/Healthy classificiation, which worked pretty well. We'll try that first, see the results, and then from there decide how to modify (if necessary) in order to get better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51e297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_model = Sequential()\n",
    "\n",
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting\n",
    "CNN_model.add(Conv2D(4, kernel_size=(8, 8), activation = 'relu', input_shape = (256, 256, 3)))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Conv2D(8, (8, 8), activation='relu'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the output of our convolutional layers\n",
    "CNN_model.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "CNN_model.add(Dense(128, activation='relu'))\n",
    "CNN_model.add(Dense(64, activation='relu'))\n",
    "CNN_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Print out a summary of the network\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3e2c3",
   "metadata": {},
   "source": [
    "About 3.4 million parameters, which is about hte same number as for the simple CNN we used for Diseased/Healthy classification, however now we have 10 categories. We'll now compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the desired loss function, optimizer, and metric(s) to track\n",
    "CNN_model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c481e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model on the training data, validating with the validation data:\n",
    "CNN_model.fit(train_generator,\n",
    "                    epochs=10,\n",
    "                    verbose = 1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = CNN_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07180627",
   "metadata": {},
   "source": [
    "Model is very overfit (with a test accuracy of only 46.9%, a val accuracy of 45%, and a train accuracy of 71.5%). Maybe it is not robust enough in order to classify the 10 different categories; we can try adding more parameters to the model (add more width in the dense layers), and also adding in more dropout in order to potentially reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e7351",
   "metadata": {},
   "source": [
    "### 2nd try at a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3d2e8",
   "metadata": {},
   "source": [
    "Here, we made the last two dense layers before the final layer twice as wide (in order to increase expressivity), and added 25% dropout to these same two layers (in order to reduce overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model_2 = Sequential()\n",
    "\n",
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting\n",
    "CNN_model_2.add(Conv2D(4, kernel_size=(8, 8), activation = 'relu', input_shape = (256, 256, 3)))\n",
    "CNN_model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model_2.add(Dropout(0.25))\n",
    "\n",
    "CNN_model_2.add(Conv2D(8, (8, 8), activation='relu'))\n",
    "CNN_model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model_2.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the output of our convolutional layers\n",
    "CNN_model_2.add(Flatten())\n",
    "\n",
    "# Add dense layers with Dropout\n",
    "CNN_model_2.add(Dense(256, activation='relu'))\n",
    "CNN_model_2.add(Dropout(0.25))\n",
    "CNN_model_2.add(Dense(128, activation='relu'))\n",
    "CNN_model_2.add(Dropout(0.25))\n",
    "CNN_model_2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Print out a summary of the network\n",
    "CNN_model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21987570",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model_2.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model on the training data, validating with the validation data:\n",
    "history_2 = CNN_model_2.fit(train_generator,\n",
    "                    epochs=20,\n",
    "                    verbose = 1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad50d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = CNN_model_2.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15966177",
   "metadata": {},
   "source": [
    "We see that we have a test accuracy of 74.9%, and a train accuracy of 72.66%, neither of which is very good, but at least overfitting is reduced. \n",
    "\n",
    "Things to try:\n",
    "- Don't do as much data augmentation? No shear, zoom, or horizontal flip, and only small angle rotation. The augmentation may be throwing things off, since all of the leaves are roughly the same shape.\n",
    "\n",
    "- Make the convolutional kernel size 3x3. It is a recommended size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2796d45",
   "metadata": {},
   "source": [
    "However, since the runs are taking a long time, we expect that it will be difficult to add more power to our model and still have it run in a reasonble time. Therefore, in order to have more modelling power with limited computational capacity, we can also try and use transfer learning in order to identify the various labels for the `Tomato` species. We will use the `ResNet50` in Keras (which expects a 224 * 224 image size) for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011895b4",
   "metadata": {},
   "source": [
    "### Setting up ResNet50 model for `Tomato` diseases identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d16337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "# only include small rotations, so that the shape detection\n",
    "# isn't thrown off too much, but also so you don't have much \n",
    "# overfitting.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range = 5)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled_tomato/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled_tomato/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# set the batch size to the number of images in the test directory:\n",
    "test_generator=test_datagen.flow_from_directory('resampled_tomato/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=2000,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "model = ResNet50V2(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(height,width,channels))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers in the base model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451fa63",
   "metadata": {},
   "source": [
    "We'll try the simplest addition, which is to just take the output of the ResNet50 into the 10 different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17975cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "output = Dense(10,activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, \n",
    "              outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbb059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1d9ac",
   "metadata": {},
   "source": [
    "From here we see that we have about 1 million trainable parameters. We can now try to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Use an early stopping callback to stop training\n",
    "# once we no longer have improvements in our validation loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=2, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data = validation_generator,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00c2ab",
   "metadata": {},
   "source": [
    "### Test Accuracy for ResNet50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6519d5df",
   "metadata": {},
   "source": [
    "We see that we have high accuracy (86.4% test accuracy and 95.77% train accuracy), but clearly have overfitting. We can try the same model, but add in a single dense layer with some dropout, and see if that can reduce the overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e866674",
   "metadata": {},
   "source": [
    "### ResNet50 again, but with added dense layer with dropout (to try and reduce the overfitting) (no good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88417207",
   "metadata": {},
   "source": [
    "In order to correct for the overfitting, we can try to add a dense layer with dropout before the final classification layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c239b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = ResNet50V2(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(height,width,channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers in the base model\n",
    "for layer in model_2.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_2.output\n",
    "x = Flatten()(x)\n",
    "# add a Dense layer with dropout in order to counteract the overfitting\n",
    "x = Dense(16,activation = 'relu')(x)\n",
    "x = Dropout(.25)(x)\n",
    "output = Dense(10,activation = 'softmax')(x)\n",
    "\n",
    "model_2 = Model(inputs=model_2.input, \n",
    "              outputs=output)\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Use an early stopping callback to stop training\n",
    "# once we no longer have improvements in our validation loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=2, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "model_2.fit(train_generator,\n",
    "          epochs=7,\n",
    "          validation_data = validation_generator,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d1a06",
   "metadata": {},
   "source": [
    "We cancelled the run due to the very low accuracy on the first two epochs; clearly, something is off when we added the other dense layer. The accuracy seems to be pretty low, so another idea is to try the original model, but introduce more rotation into the train data, the validation data, and the test data, in order to try and overcome overfitting. In order to do so, we want to introduce a function that allows us to reset the model without having to reinitialize it everytime, so that we can try various data augmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(train, val, X_test, y_test, model, \n",
    "                   epoch_n, loss_ = 'categorical_crossentropy', \n",
    "                   optimizer_ = 'Adam'):\n",
    "    \n",
    "\n",
    "    ''' This function allows for retraining the same model multiple \n",
    "        times, without having issues about having to re-initialize \n",
    "        (re-instantiate, recompile, and refit) the model everytime \n",
    "        you run it. You can just instantiate the model once, and then \n",
    "        use this function as many times as necessary. This is useful \n",
    "        for testing various types of data augmentation.'''\n",
    "    \n",
    "    # compile for initialization of the loss \n",
    "    # and optimization\n",
    "    model.compile(loss = loss_,\n",
    "                  optimizer = optimizer_,\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    # get initial weights\n",
    "    weights = model.get_weights()\n",
    "    \n",
    "    #early_stop = EarlyStopping(monitor='val_loss', \n",
    "    #                       patience=2, \n",
    "    #                       mode='min', \n",
    "    #                       verbose=1)\n",
    "    \n",
    "    # get the model fit\n",
    "    model_fit = model.fit(train,\n",
    "                          epochs=epoch_n,\n",
    "                          validation_data = val,\n",
    "                          #callbacks=[early_stop],\n",
    "                          shuffle = True)\n",
    "    \n",
    "    # weights and optimizer state after training\n",
    "    trained_weights = model.get_weights()\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # reset to the initialized weights\n",
    "    model.set_weights(weights)\n",
    "\n",
    "    return Y_pred, trained_weights, score, model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb87424",
   "metadata": {},
   "source": [
    "### Trying out new function `train_and_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396ec9f",
   "metadata": {},
   "source": [
    "We first will try adding zero data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled_tomato/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled_tomato/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# set the batch size to the number of images in the test directory:\n",
    "test_generator=test_datagen.flow_from_directory('resampled_tomato/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=2000,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84165c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = train_and_test(train_generator, \n",
    "                       validation_generator, \n",
    "                       X_test, \n",
    "                       y_test, \n",
    "                       model, \n",
    "                       7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f87cc",
   "metadata": {},
   "source": [
    "Added the `shuffle = True` option to the `test_and_train` function, see if that improves anything, and additionally add 30 degrees of rotation range in the `train_datagen`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range = 30)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled_tomato/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled_tomato/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# set the batch size to the number of images in the test directory:\n",
    "test_generator=test_datagen.flow_from_directory('resampled_tomato/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=2000,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_2 = train_and_test(train_generator, \n",
    "                       validation_generator, \n",
    "                       X_test, \n",
    "                       y_test, \n",
    "                       model, \n",
    "                       7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32364cf",
   "metadata": {},
   "source": [
    "Still have the overfitting. Trying again with lots of augmentation in the `train_datagen`, in addition to the shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range = 30,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled_tomato/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled_tomato/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# set the batch size to the number of images in the test directory:\n",
    "test_generator=test_datagen.flow_from_directory('resampled_tomato/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=2000,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_3 = train_and_test(train_generator, \n",
    "                       validation_generator, \n",
    "                       X_test, \n",
    "                       y_test, \n",
    "                       model, \n",
    "                       7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa1b2f",
   "metadata": {},
   "source": [
    "Seems more promising, can try without the early stopping and see if it gets any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846032af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_4 = train_and_test(train_generator, \n",
    "                       validation_generator, \n",
    "                       X_test, \n",
    "                       y_test, \n",
    "                       model, \n",
    "                       6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47bbbd",
   "metadata": {},
   "source": [
    "89% test accuracy, 91% train accuracy seems to be the best result obtained in terms of balancing the overall accuracy with minimizing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee8c86",
   "metadata": {},
   "source": [
    "We will go with the `trial_4` iteration of the model, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4bcdd",
   "metadata": {},
   "source": [
    "- `trial_4[0]` gives the `Y-pred` values\n",
    "- `trial_4[1]` gives the `trained_weights` of the ResNet50 model\n",
    "- `trial_4[2]` gives the `score` of the model\n",
    "- `trial_4[3]` gives the `model_fit` of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b2e2f",
   "metadata": {},
   "source": [
    "Taking `trial_4`, we can now do a confusion matrix analysis in order to dig into the accuracy, precision, and recall for each class of the `Tomato` species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254aa59",
   "metadata": {},
   "source": [
    "We need to convert `predict_probas` and `y_test` into label encoded form for easier analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb41fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions\n",
    "predict_probas = trial_4[0]\n",
    "\n",
    "# Convert probabilities to label encoding\n",
    "y_predict = np.argmax(predict_probas, axis=1)\n",
    "\n",
    "# convert test labels from OHE to LE:\n",
    "y_test_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136ed04",
   "metadata": {},
   "source": [
    "We recall that a binary confusion matrix allows us to see the True Positives (correct classification), True Negatives (the correct classification), False Positives (incorrect classification), and the False Negatives (incorrect classification). As a refresher:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd3a08",
   "metadata": {},
   "source": [
    "1. **True Positive (TP)**: Correctly predict that $x$ belongs to the positive class\n",
    "2. **False Positive (FP)**: Incorrectly predict that $x$  belongs to the positive class\n",
    "3. **True Negative (TN)**: Correctly predict $x$'s membership in for the negative class\n",
    "4. **False Negative (FN)**: Incorrectly predict $x$'s membership in for the negative class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43f6f9",
   "metadata": {},
   "source": [
    "Now we want to plot the confusion matrix for the 10 different classes among the `Tomato` species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8ac4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define confusion matrix\n",
    "conf_mat = confusion_matrix(y_test_labels, y_predict)\n",
    "normalized_conf_mat = conf_mat / conf_mat.sum(axis=1)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(normalized_conf_mat,\n",
    "            annot=True,\n",
    "            cbar=False,\n",
    "            xticklabels=label_names,\n",
    "            yticklabels=label_names,\n",
    "            cmap=\"rocket_r\",\n",
    "            linewidths=1,\n",
    "            \n",
    "           )\n",
    "plt.title('Species: Tomato Confusion Matrix',size = 25,y=1.01)\n",
    "plt.xlabel(\"Predicted Label\", size = 20)\n",
    "plt.ylabel(\"True Label\", size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0bfe4",
   "metadata": {},
   "source": [
    "We note that the structure of this multi-class confusion matrix is such that correct classifications are on the diagonal, and for a given category, all the elements in a given row (other than the diagonal element) represent all the false negatives (FNs) for the class of that row, while all the elements in a column (other than the diagonal element) represent a false positives (FNs) for the class of that column. The true positive (TP) is the diagonal element, and the true negatives (TNs) consist of all of the items that aren't a given class and aren't incorrectly classified as that given class. From this, we can talk about precision and recall:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64cfd0",
   "metadata": {},
   "source": [
    "Precision measures what proportion of a model assigned to positive are actually members of the positive class:\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "On the other hand, recall measures how many members of the positive class the model correctly identified out of the total positives:\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825542f1",
   "metadata": {},
   "source": [
    "We can now print out the classification report, which gives us the precision and recall for each class (and the f1 score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520be78d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_initial = classification_report(y_test_labels, y_predict)\n",
    "print(report_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425801d3",
   "metadata": {},
   "source": [
    "From the above report, we see that lowest 3 precision scores go to: \n",
    "- (1) class 6 (which corresponds to `Tomato___Target_Spot`) at 73%, \n",
    "- (2) class 1 (which corresponds to `Tomato___Early_blight`) at 83%, and \n",
    "- (3) class 4 (which corresponds to `Tomato___Septoria_leaf_spot`) at 84%.\n",
    "\n",
    "Correspondingly, we see that the lowest three recall scores go to: \n",
    "- (1) class 2 (which corresponds to `Tomato___Late_blight`) at 79%, \n",
    "- (2) class 4 (which corresponds to `Tomato___Septoria_leaf_spot`) at 83%, and \n",
    "- (3) class 1 (which corresponds to `Tomato___Early_blight`) at 84%.\n",
    "\n",
    "This means that categories are most commonly incorrectly labelled as  the diseases `Target_Spot`, `Early_blight`, and `Septoria_leaf_spot` respectively (which corresponds to the precision score being low for these respective categories).\n",
    "\n",
    "Additionally, the categories that are most commonly incorrectly labelled are `Late_blight`, `Septoria_leaf_spot`, and `Early_blight`, respectively (which corresponds to the recall score being low for these respective categories).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bca434",
   "metadata": {},
   "source": [
    "From a visual examination of several samples of the representative images of the classes above (from the `Data Manipulation` section), we see that `Target_spot` is very subtle, so it makes sense that it could be misused as a label frequently (other spotted things, healthy could be mislabelled as `Target_spot` easily), explaining why it's precision is so low. Also, `Septoria` and `Early_blight` look interchangable in many cases, which could explain why they also are used as labels incorrectly most frequently. \n",
    "\n",
    "For the recall, the `Late_blight` varies a lot (changes colors, extent, etc.), so it makes sense that it is labelled correctly most infrequently. Similarly, since `Early_blight` and `Septoria` are pretty similar, they may be mislabelled as each other often (which can be seen in the confusion matrix). We also see from the confusion matrix that `Late_blight` and `Early_blight` are mixed up frequently, which is unsuprising.\n",
    "\n",
    "We also note that `Two_Spotted_spider_mite` is very frequently (10% of the time!) misidentified as `Target_spot`. These observations may be useful heuristics for constructing a more accurate classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dddc9f",
   "metadata": {},
   "source": [
    "We can further look at a sampling (sample of 9) of misidentified photos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a49736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from random import sample\n",
    "\n",
    "# just have the disease names\n",
    "disease_names = [label_names[i].split(\"___\")[1] for i in range(len(label_names))]\n",
    "\n",
    "# incorrect photo index\n",
    "incorrect_photos = y_test_labels != y_predict\n",
    "\n",
    "\n",
    "random_sample_incorrect_photos = sample(list(range(X_test[incorrect_photos].shape[0])),9)\n",
    "list_random_sample = [X_test[incorrect_photos][num] for num in random_sample_incorrect_photos]\n",
    "y_predict_random_sample = [y_predict[incorrect_photos][num] for num in random_sample_incorrect_photos]\n",
    "y_test_labels_random_sample = [y_test_labels[incorrect_photos][num] for num in random_sample_incorrect_photos]\n",
    "\n",
    "\n",
    "num_images = np.count_nonzero(incorrect_photos)\n",
    "columns = 3\n",
    "rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=(9, 14))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(0,len(list_random_sample)):\n",
    "    axes[i].imshow(list_random_sample[i], cmap='gray')\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_label = disease_names[y_predict_random_sample[i]]\n",
    "    \n",
    "    # Get actual label\n",
    "    true_label =  disease_names[y_test_labels_random_sample[i]]\n",
    "    \n",
    "    # Set the title of the subplot\n",
    "    axes[i].set_title(f\"Predicted: {predicted_label}\\n True: {true_label}\")\n",
    "    \n",
    "    # Hide the x and y ticks\n",
    "    axes[i].set_xticks([]) \n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "# print statements:\n",
    "print(f\"Total number of incorrectly identified photos: {num_images} out of {len(y_predict)}, {100*round(num_images/len(y_predict),2)}%.\")\n",
    "print(\"Here is a sample of 9 incorrectly identified photos:\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ccecc",
   "metadata": {},
   "source": [
    "Can also look at the ROC curves in order to gain more insight on the classification of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970d97f",
   "metadata": {},
   "source": [
    "### ROC curves "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954f455",
   "metadata": {},
   "source": [
    "ROC curves allow us to look at the true positive rate (TPR = TP/(TP+FN)) and the false positive rate (FPR = FP/(FP+TN)) as a function of a threshold for hard classification. Usually this is used only for binary classification, but we can generalize to multi-class classification by comparing the ROC curve of a given class vs all other classes (called One versus Rest (OvR)). From this, we can see how much the classification depends on the threshold; if the classification (defined as a function of the TPR and the FPR) is largely independent of the threshold, that is a desirable outcome, as it means that the classification is mostly insensitive to where the threshold is, so we don't have to worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda17082",
   "metadata": {},
   "source": [
    "To do the multi-class analysis, we take definitions from the following blog for OvR:\n",
    "https://towardsdatascience.com/multiclass-classification-evaluation-with-roc-curves-and-roc-auc-294fd4617e3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(y_real, y_pred):\n",
    "    '''\n",
    "    Calculates the True Positive Rate (tpr) and the True Negative Rate (fpr) based on real and predicted observations\n",
    "    \n",
    "    Args:\n",
    "        y_real: The list or series with the real classes\n",
    "        y_pred: The list or series with the predicted classes\n",
    "        \n",
    "    Returns:\n",
    "        tpr: The True Positive Rate of the classifier\n",
    "        fpr: The False Positive Rate of the classifier\n",
    "    '''\n",
    "    \n",
    "    # Calculates the confusion matrix and recover each element\n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "    \n",
    "    # Calculates tpr and fpr\n",
    "    tpr =  TP/(TP + FN) # sensitivity - true positive rate\n",
    "    fpr = 1 - TN/(TN+FP) # 1-specificity - false positive rate\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_roc_coordinates(y_real, y_proba):\n",
    "    '''\n",
    "    Calculates all the ROC Curve coordinates (tpr and fpr) by considering each point as a treshold for the predicion of the class.\n",
    "    \n",
    "    Args:\n",
    "        y_real: The list or series with the real classes.\n",
    "        y_proba: The array with the probabilities for each class, obtained by using the `.predict_proba()` method.\n",
    "        \n",
    "    Returns:\n",
    "        tpr_list: The list of TPRs representing each threshold.\n",
    "        fpr_list: The list of FPRs representing each threshold.\n",
    "    '''\n",
    "    tpr_list = [0]\n",
    "    fpr_list = [0]\n",
    "    for i in range(len(y_proba)):\n",
    "        threshold = y_proba[i]\n",
    "        y_pred = y_proba >= threshold\n",
    "        tpr, fpr = calculate_tpr_fpr(y_real, y_pred)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    return tpr_list, fpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(tpr, fpr, scatter = True, ax = None):\n",
    "    '''\n",
    "    Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "    Args:\n",
    "        tpr: The list of TPRs representing each coordinate.\n",
    "        fpr: The list of FPRs representing each coordinate.\n",
    "        scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "    '''\n",
    "    if ax == None:\n",
    "        plt.figure(figsize = (5, 5))\n",
    "        ax = plt.axes()\n",
    "    \n",
    "    if scatter:\n",
    "        sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = fpr, y = tpr, ax = ax)\n",
    "    sns.lineplot(x = [0, 1], y = [0, 1], color = 'green', ax = ax)\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835353ad",
   "metadata": {},
   "source": [
    "Plot the ROC curves over a range (since it is hard to get all 10 columns in a single frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC_OvR(start, end):   \n",
    "    plt.figure(figsize = (16,8))\n",
    "    bins = [i/20 for i in range(20)] + [1]\n",
    "    classes = range(len(label_names))\n",
    "    roc_auc_ovr = {}\n",
    "    for i in range(start-1, end):\n",
    "        # Gets the class\n",
    "        c = classes[i]\n",
    "        name = label_names[i]\n",
    "\n",
    "        # Prepares an auxiliar dataframe to help with the plots\n",
    "        df_aux = pd.DataFrame()\n",
    "        df_aux['class'] = [1 if y == c else 0 for y in y_test_labels]\n",
    "        df_aux['prob'] = predict_probas[:, i]\n",
    "        df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "        # Plots the probability distribution for the class and the rest\n",
    "        ax = plt.subplot(2, end-(start-1), i-start+2)\n",
    "        sns.histplot(x = \"prob\", data = df_aux, hue = 'class', color = 'b', ax = ax, bins = bins)\n",
    "        ax.set_title(name)\n",
    "        ax.legend([f\"Class: {name}\", \"Rest\"])\n",
    "        ax.set_xlabel(f\"P(x = {name})\")\n",
    "\n",
    "        # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "        ax_bottom = plt.subplot(2, end-(start-1), i+(end-(start-1))-start+2)\n",
    "        tpr, fpr = get_all_roc_coordinates(df_aux['class'], df_aux['prob'])\n",
    "        plot_roc_curve(tpr, fpr, scatter = False, ax = ax_bottom)\n",
    "        ax_bottom.set_title(\"ROC Curve OvR\")\n",
    "\n",
    "        # Calculates the ROC AUC OvR\n",
    "        roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108101f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of the first 4 ROC curves and class distributions (OvR):\n",
    "plot_ROC_OvR(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf756eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot of the ROC curves and class distributions (OvR) for classes 5-7:\n",
    "plot_ROC_OvR(5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e63072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the ROC curves and class distributions (OvR) for classes 8-10:\n",
    "plot_ROC_OvR(8,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4fb30e",
   "metadata": {},
   "source": [
    "We see from the above plots that the class probability distributions (of a single class vs. all others) are clearly well separated, which in turn translates to the classification being largely independent of where we set the classification threshold to be, which is also reflected in the ROC curves. In particular, a ROC curve with an area under the curve (AUC) of 1 would lead to a perfect separation of the classes, while an AUC of 0.5 would be as good as flipping a coin. Below, we print the AUC scores for the classificaiton problems of classifying a particular class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8793d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the AUC values for each ROC curve:\n",
    "classes = range(len(label_names))\n",
    "roc_auc_ovr = {}\n",
    "for i in range(len(label_names)):\n",
    "    # Gets the class\n",
    "    c = classes[i]\n",
    "    name = label_names[i]\n",
    "\n",
    "    # Prepares an auxiliar dataframe to help with the plots\n",
    "    df_aux = pd.DataFrame()\n",
    "    df_aux['class'] = [1 if y == c else 0 for y in y_test_labels]\n",
    "    df_aux['prob'] = predict_probas[:, i]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    roc_auc_ovr[c] = roc_auc_score(df_aux['class'], df_aux['prob'])\n",
    "\n",
    "# print out the ROC-AUC scores:\n",
    "# Displays the ROC AUC for each class\n",
    "avg_roc_auc = 0\n",
    "i = 0\n",
    "for k in roc_auc_ovr:\n",
    "    avg_roc_auc += roc_auc_ovr[k]\n",
    "    i += 1\n",
    "    print(f\"{label_names[k]} ROC AUC OvR: {roc_auc_ovr[k]:.4f}\")\n",
    "print(f\"average ROC AUC OvR: {avg_roc_auc/i:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977cc46",
   "metadata": {},
   "source": [
    "From the above, we see that all of the ROC AUC scores are very high (greater than 98% in all cases), meaning that the classification accuracy is largely independent of where a classification threshold is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9722d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(['Bacterial_spot',\n",
    "                'Early_blight',\n",
    "                'Late_blight',\n",
    "                'Leaf_Mold',\n",
    "                'Septoria_leaf_spot',\n",
    "                'Spider_mites Two-spotted_spider_mite',\n",
    "                'Target_Spot',\n",
    "                'Tomato_Yellow_Leaf_Curl_Virus',\n",
    "                'Tomato_mosaic_virus',\n",
    "                'healthy'], [88,85,79,89,83,88,93,97,99,89])\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(\"Percentage Accuracy for Tomato Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dff83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

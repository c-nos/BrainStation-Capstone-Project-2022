{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3b7a03",
   "metadata": {},
   "source": [
    "# Capstone Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c757e",
   "metadata": {},
   "source": [
    "## Casey Nosiglia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8314423",
   "metadata": {},
   "source": [
    "In this notebook, we will do the data manipulation that will allow us to create directories for our business questions of (1) determining if a leaf is diseased or not, and (2) determining if a given species of plant (e.g., `Tomato`) is diseased or not, and if it is diseased, what kind of disease does it have. The dataset is the plant disease dataset from Kaggle: https://www.kaggle.com/datasets/saroz014/plant-disease\n",
    "This dataset includes ~54.3k image files at about 2GB, with 12 different species, and 38 different classes in total.\n",
    "\n",
    "This notebook is meant to be operated from the same directory as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4b36f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if this line is run, it will give all the version requirements of the libraries loaded in.\n",
    "# This may be a useful reference for later\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all of the relevant libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a2b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9632b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image handling libraries\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cf3cd",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0b297",
   "metadata": {},
   "source": [
    "We write down the lists `class_names_list` and `species_list` for now, as they wil be useful for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2191878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all of the different class names (38 in total):\n",
    "class_names_list = ['Apple___Apple_scab',\n",
    "                    'Apple___Black_rot',\n",
    "                    'Apple___Cedar_apple_rust',\n",
    "                    'Apple___healthy',\n",
    "                    'Blueberry___healthy',\n",
    "                    'Cherry_(including_sour)___healthy',\n",
    "                    'Cherry_(including_sour)___Powdery_mildew',\n",
    "                    'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
    "                    'Corn_(maize)___Common_rust_',\n",
    "                    'Corn_(maize)___healthy',\n",
    "                    'Corn_(maize)___Northern_Leaf_Blight',\n",
    "                    'Grape___Black_rot',\n",
    "                    'Grape___Esca_(Black_Measles)',\n",
    "                    'Grape___healthy',\n",
    "                    'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
    "                    'Orange___Haunglongbing_(Citrus_greening)',\n",
    "                    'Peach___Bacterial_spot',\n",
    "                    'Peach___healthy',\n",
    "                    'Pepper,_bell___Bacterial_spot',\n",
    "                    'Pepper,_bell___healthy',\n",
    "                    'Potato___Early_blight',\n",
    "                    'Potato___healthy',\n",
    "                    'Potato___Late_blight',\n",
    "                    'Raspberry___healthy',\n",
    "                    'Soybean___healthy',\n",
    "                    'Squash___Powdery_mildew',\n",
    "                    'Strawberry___healthy',\n",
    "                    'Strawberry___Leaf_scorch',\n",
    "                    'Tomato___Bacterial_spot',\n",
    "                    'Tomato___Early_blight',\n",
    "                    'Tomato___healthy',\n",
    "                    'Tomato___Late_blight',\n",
    "                    'Tomato___Leaf_Mold',\n",
    "                    'Tomato___Septoria_leaf_spot',\n",
    "                    'Tomato___Spider_mites Two-spotted_spider_mite',\n",
    "                    'Tomato___Target_Spot',\n",
    "                    'Tomato___Tomato_mosaic_virus',\n",
    "                    'Tomato___Tomato_Yellow_Leaf_Curl_Virus']\n",
    "len(class_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f5dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of all the different species of plants\n",
    "species_list = ['Apple',\n",
    "                'Blueberry',\n",
    "                'Cherry_(including_sour)',\n",
    "                'Corn_(maize)',\n",
    "                'Grape',\n",
    "                'Orange',\n",
    "                'Peach',\n",
    "                'Pepper,_bell',\n",
    "                'Potato', \n",
    "                'Raspberry',\n",
    "                'Soybean', \n",
    "                'Squash',\n",
    "                'Strawberry',\n",
    "                'Tomato']\n",
    "\n",
    "len(species_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fffd1",
   "metadata": {},
   "source": [
    "We can define some useful functions in order to manipulate the `dataset` directory, in order to reorder the data for our particular questions that we want to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18002f9e",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183b92a",
   "metadata": {},
   "source": [
    "### ID Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4af139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which gives a full list of the filepath names \n",
    "# (these are the unique identifiers of the images):\n",
    "def filepath_names(test_or_train):\n",
    "    filepath_name = []\n",
    "    for i in range(0,len(class_names_list)):\n",
    "        for f in glob.iglob(\"dataset/\" + test_or_train + '/' + class_names_list[i] + '/*'):\n",
    "            filepath_name.append(f)\n",
    "    return filepath_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12282106",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filepath_names(\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90cc76",
   "metadata": {},
   "source": [
    "### Is Diseased function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6770020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which gives a list of binary indicators of dieased (1) or not (0):\n",
    "def diseased(test_or_train):\n",
    "    is_diseased = []\n",
    "    for i in range(0,len(class_names_list)):\n",
    "        for f in glob.iglob(\"dataset/\" + test_or_train + '/' + class_names_list[i] + '/*'):\n",
    "            \n",
    "            # if the string has healthy in it\n",
    "            if \"healthy\" in class_names_list[i]:\n",
    "                # not diseased\n",
    "                is_diseased.append(0)\n",
    "            else:\n",
    "                # diseased\n",
    "                is_diseased.append(1)\n",
    "                \n",
    "    return is_diseased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diseased(\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b426a",
   "metadata": {},
   "source": [
    "### Species Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which gives the species type:\n",
    "def species(test_or_train):\n",
    "    species_type = []\n",
    "    for i in range(0,len(class_names_list)):\n",
    "        for f in glob.iglob(\"dataset/\" + test_or_train + '/' + class_names_list[i] + '/*'):\n",
    "            \n",
    "            # return the disease name if it isn't healthy:\n",
    "            for j in range(0, len(species_list)):\n",
    "                if species_list[j] in class_names_list[i]:\n",
    "                    species_type.append(species_list[j])\n",
    "    return species_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a784d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(species(\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5ffd9",
   "metadata": {},
   "source": [
    "### Disease Type Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which gives the disease type:\n",
    "def disease(test_or_train):\n",
    "    disease_type = []\n",
    "    for i in range(0,len(class_names_list)):\n",
    "        for f in glob.iglob(\"dataset/\" + test_or_train + '/' + class_names_list[i] + '/*'):\n",
    "            \n",
    "            # iterate over species:\n",
    "            for j in range(0, len(species_list)):\n",
    "                # check if not healthy and if a given species:\n",
    "                if (species_list[j] in class_names_list[i]) & (\"healthy\" not in class_names_list[i]):\n",
    "                    # split the part off after the species name\n",
    "                    disease_type.append(class_names_list[i].split(\"___\")[1])\n",
    "                elif (species_list[j] in class_names_list[i]) & (\"healthy\" in class_names_list[i]):\n",
    "                    disease_type.append(\"healthy\")\n",
    "    return disease_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b3754",
   "metadata": {},
   "source": [
    "Using the above functions, we want to create a dataframe containing information of the filepath, the disease status, the species, and the disease type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b63d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for the train data.\n",
    "train_dataframe = pd.DataFrame({'ID':filepath_names(\"train\"), \n",
    "                                'is_diseased':diseased(\"train\"),\n",
    "                                'Species':species(\"train\"),\n",
    "                                'Disease_Type':disease(\"train\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118d709",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataframe.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b72c8",
   "metadata": {},
   "source": [
    "We want to do the same for the test data, and then we can concatenate both to get the final dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = pd.DataFrame({'ID':filepath_names(\"test\"), \n",
    "                                'is_diseased':diseased(\"test\"),\n",
    "                                'Species':species(\"test\"),\n",
    "                                'Disease_Type':disease(\"test\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two to get the full dataset\n",
    "full_dataframe = pd.concat([train_dataframe, test_dataframe], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex\n",
    "full_dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4532181",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_dataframe.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c64ba6f",
   "metadata": {},
   "source": [
    "We can look at the full counts of the data by `Disease_Type` and `Species`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe[['Disease_Type','Species']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da526",
   "metadata": {},
   "source": [
    "We can also look at the diseased/healthy rate in a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba307fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# diseased vs healthy plot:\n",
    "plt.figure(figsize = (8,8))\n",
    "graph = plt.bar([\"Diseased\",'Healthy'],full_dataframe['is_diseased'].value_counts(normalize=True))\n",
    "plt.xlabel('Disease Status')\n",
    "plt.ylabel(\"Percentage of total\")\n",
    "plt.title(\"Percentage of Total Diseased vs Healthy\")\n",
    " \n",
    "i = 0\n",
    "for p in graph:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    plt.text(x+width/2,\n",
    "             y+height*1.01,\n",
    "             str(round(100*full_dataframe['is_diseased'].value_counts(normalize=True)[1-i],2))+'%',\n",
    "             ha='center')\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diseased vs healthy plot (raw numbers):\n",
    "plt.figure(figsize = (8,8))\n",
    "graph = plt.bar([\"Diseased\",'Healthy'],full_dataframe['is_diseased'].value_counts())\n",
    "plt.xlabel('Disease Status')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Total Diseased vs Healthy\")\n",
    "\n",
    "i = 0\n",
    "for p in graph:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    plt.text(x+width/2,\n",
    "             y+height*1.01,\n",
    "             str(round(100*full_dataframe['is_diseased'].value_counts(normalize=False)[1-i],2)),\n",
    "             ha='center')\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81d79e",
   "metadata": {},
   "source": [
    "We see that it is about 70/30, which is an ok split, but we can manipulate the data more later in order to get an even split there for the classification task of Healthy\\Diseased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d8bd2",
   "metadata": {},
   "source": [
    "We can further look at the species distribution, the healthy species distirbution, and the diseased species distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd113c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(full_dataframe['Species'].value_counts().index,full_dataframe['Species'].value_counts(normalize = True))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel(\"Percentage of total\")\n",
    "plt.title(\"Species Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(full_dataframe[full_dataframe['is_diseased'] == 0]['Species'].value_counts().index,full_dataframe[full_dataframe['is_diseased'] == 0]['Species'].value_counts(normalize = True))\n",
    "plt.xlabel('Species')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Percentage of total\")\n",
    "plt.title(\"Healthy Species Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ed36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(full_dataframe[full_dataframe['is_diseased'] == 1]['Species'].value_counts().index,full_dataframe[full_dataframe['is_diseased'] == 1]['Species'].value_counts(normalize = True))\n",
    "plt.xlabel('Species')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Percentage of total\")\n",
    "plt.title(\"Diseased Species Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a7a5f",
   "metadata": {},
   "source": [
    "We see that `Tomato` is dominant in the species distribution, so we want to look at the class distribution for just `Tomato`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bab995",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "plt.bar(full_dataframe[full_dataframe['Species']=='Tomato']['Disease_Type'].value_counts().index,full_dataframe[full_dataframe['Species']=='Tomato']['Disease_Type'].value_counts(normalize = True))\n",
    "plt.xlabel('Species')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.ylabel(\"Percentage of total\")\n",
    "plt.title(\"Tomato Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482d66e",
   "metadata": {},
   "source": [
    "## Creating the image directories for diseased/healthy and all classes (we end up not using the latter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc2629",
   "metadata": {},
   "source": [
    "For diseased vs not diseased, we only want to have the info of whether or not it is diseased and the image file. So we can do a train-test split this way. Want to get the filepaths for iterating over for both the test and the train sub-directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_fulldf = []\n",
    "for i in range(0,len(full_dataframe)):\n",
    "    filepaths_fulldf.append(full_dataframe['ID'][i][0:full_dataframe['ID'][i].rfind('/')+1]+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique filepaths\n",
    "filepaths_fulldf_unique = []\n",
    "[filepaths_fulldf_unique.append(x) for x in filepaths_fulldf if x not in filepaths_fulldf_unique];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43407513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepaths_fulldf_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61935329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(filepaths_fulldf_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff5809",
   "metadata": {},
   "source": [
    "This makes sense, since we have 38 different categories for test and for train (38+38 = 76)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d0200",
   "metadata": {},
   "source": [
    "We want to reduce the number of images so that we can tractably model the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ab357",
   "metadata": {},
   "source": [
    "### Downsampling and upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to save the full_dataframe to  csv so that we can access it from other notebooks:\n",
    "full_dataframe.to_csv(\"full_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec256c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataframe[['Species', 'Disease_Type']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0007c17",
   "metadata": {},
   "source": [
    "We will downsample larger categories down to 750, and upsample smaller categories to 750, in order to have a more tractable dataset of 750*38 = 28,500 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices for the various categories\n",
    "spec_dis_list = full_dataframe[['Species', 'Disease_Type']].value_counts().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac67399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled = pd.DataFrame(columns = ['ID', 'is_diseased', 'Species','Disease_Type'])\n",
    "\n",
    "#resampling loop (including randomized downsampling and upsampling):\n",
    "for i in range(0,len(spec_dis_list)):\n",
    "    # downsampling\n",
    "    if full_dataframe.loc[(full_dataframe['Species'] == spec_dis_list[i][0]) & \n",
    "                          (full_dataframe['Disease_Type'] == spec_dis_list[i][1])].shape[0] > 750:\n",
    "        resampled = pd.concat([resampled, full_dataframe.loc[(full_dataframe['Species'] == spec_dis_list[i][0]) & \n",
    "                          (full_dataframe['Disease_Type'] == spec_dis_list[i][1])].sample(750)],\n",
    "              axis = 0)\n",
    "    # upsampling    \n",
    "    else: \n",
    "        resampled = pd.concat([resampled, full_dataframe.loc[(full_dataframe['Species'] == spec_dis_list[i][0]) & \n",
    "                          (full_dataframe['Disease_Type'] == spec_dis_list[i][1])].sample(750, replace = True)],\n",
    "              axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab64b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled[['Species', 'Disease_Type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e707284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled['is_diseased'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85bac15",
   "metadata": {},
   "source": [
    "We also want to sample the class `is_diseased` to be more even, so we will create another dataframe that resamples along this category:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706aa457",
   "metadata": {},
   "source": [
    "### Diseased/healthy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067335ff",
   "metadata": {},
   "source": [
    "We will create a diseased/healthy dataframe (called `resampled2`) also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a857d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled2 = pd.concat([resampled.loc[resampled['is_diseased'] == 1].sample(9000), resampled.loc[resampled['is_diseased'] == 0]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f8810",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# even\n",
    "resampled2['is_diseased'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c148a21",
   "metadata": {},
   "source": [
    "We can make this into a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc37faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diseased vs healthy plot:\n",
    "plt.figure(figsize = (8,8))\n",
    "graph = plt.bar([\"Diseased\",'Healthy'],resampled2['is_diseased'].value_counts())\n",
    "plt.xlabel('Disease Status')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Count of Total Diseased vs Healthy (After Balancing)\")\n",
    " \n",
    "i = 0\n",
    "for p in graph:\n",
    "    width = p.get_width()\n",
    "    height = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    plt.text(x+width/2,\n",
    "             y+height*1.01,\n",
    "             str(round(100*resampled2['is_diseased'].value_counts(normalize=True)[1-i],1))+'%',\n",
    "             ha='center')\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd97704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled[['Species','Disease_Type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled2[['Species','Disease_Type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index for resampled\n",
    "resampled.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52449a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset index for resampled2\n",
    "resampled2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8bfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampled2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee5666b",
   "metadata": {},
   "source": [
    "We now want to make a new image directory from `resampled2` which wil be easy to manipulate for later modeling. We add a train subdirectory, a validation subdirectory, and a test subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new directory for resampled2\n",
    "! mkdir resampled2\n",
    "! mkdir resampled2/train\n",
    "! mkdir resampled2/train/diseased\n",
    "! mkdir resampled2/train/healthy\n",
    "! mkdir resampled2/test\n",
    "! mkdir resampled2/test/diseased\n",
    "! mkdir resampled2/test/healthy\n",
    "! mkdir resampled2/validation\n",
    "! mkdir resampled2/validation/diseased\n",
    "! mkdir resampled2/validation/healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f539a",
   "metadata": {},
   "source": [
    "Since we upsampled and downsampled in the creation of `resampled` and `resampled2`, we need to be able to copy the same file more than once to the new directories, so we use a special function called `safe_copy` which enables this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a383ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition taken from:\n",
    "# https://stackoverflow.com/questions/33282647/python-shutil-copy-if-i-have-a-duplicate-file-will-it-copy-to-new-location\n",
    "\n",
    "def safe_copy(file_path, out_dir, dst = None):\n",
    "    \"\"\"Safely copy a file to the specified directory. If a file with the same name already \n",
    "    exists, the copied file name is altered to preserve both.\n",
    "\n",
    "    :param str file_path: Path to the file to copy.\n",
    "    :param str out_dir: Directory to copy the file into.\n",
    "    :param str dst: New name for the copied file. If None, use the name of the original\n",
    "        file.\n",
    "    \"\"\"\n",
    "    name = dst or os.path.basename(file_path)\n",
    "    if not os.path.exists(os.path.join(out_dir, name)):\n",
    "        shutil.copy(file_path, os.path.join(out_dir, name))\n",
    "    else:\n",
    "        base, extension = os.path.splitext(name)\n",
    "        i = 1\n",
    "        while os.path.exists(os.path.join(out_dir, '{}_{}{}'.format(base, i, extension))):\n",
    "            i += 1\n",
    "        shutil.copy(file_path, os.path.join(out_dir, '{}_{}{}'.format(base, i, extension)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2130",
   "metadata": {},
   "source": [
    "   We now want to generate the train/val/test split for `resampled2`, in order to get the randomized train validation and test directories we want. We will also use stratify by the folder name in order to keep the class balance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f48331",
   "metadata": {},
   "source": [
    "We will add a `Folder` column to `resampled2` in order to stratify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ae971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder column\n",
    "spec_dis2 = [item.split(\"/\")[2] for item in resampled2['ID']]\n",
    "spec_dis2 = pd.DataFrame({\"Folder\":spec_dis2})\n",
    "\n",
    "# concatenate\n",
    "resampled2 = pd.concat([resampled2, spec_dis2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5476976",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_resampled2, test_resampled2 = train_test_split(resampled2, test_size=0.2, stratify = resampled2['Folder'])\n",
    "train_r_resampled2, val_resampled2 = train_test_split(train_resampled2, test_size=0.3, stratify = train_resampled2['Folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ad9f0",
   "metadata": {},
   "source": [
    "Now we can fill up the `resampled2` directory, which represents diseased/healthy plants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the resampled2 directory:\n",
    "import shutil\n",
    "\n",
    "train_IDs = train_r_resampled2['ID'].to_list()\n",
    "test_IDs = test_resampled2['ID'].to_list()\n",
    "val_IDs = val_resampled2['ID'].to_list()\n",
    "\n",
    "for ID in train_IDs:\n",
    "    if 'healthy' in ID:\n",
    "        safe_copy(ID, 'resampled2/train/healthy')\n",
    "    else:\n",
    "        safe_copy(ID, 'resampled2/train/diseased')\n",
    "        \n",
    "for ID in test_IDs:\n",
    "    if 'healthy' in ID:\n",
    "        safe_copy(ID, 'resampled2/test/healthy')\n",
    "    else:\n",
    "        safe_copy(ID, 'resampled2/test/diseased')\n",
    "        \n",
    "for ID in val_IDs:\n",
    "    if 'healthy' in ID:\n",
    "        safe_copy(ID, 'resampled2/validation/healthy')\n",
    "    else:\n",
    "        safe_copy(ID, 'resampled2/validation/diseased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bde0c",
   "metadata": {},
   "source": [
    "Now I can easily do data preprocessing for the CNN using Keras for the `resampled2` data, by using the `ImageDataGenerator` function, which directly loads from a directory, and which can enable batching and save a lot of memory during training of a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d7480",
   "metadata": {},
   "source": [
    "We will also create a `resampled` directory for our even split among all classes that we created. To do so, first we will create a list of the folder names so that we can easily make the directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9222829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the desired file paths:\n",
    "folders = [spec_dis_list[i][0]+'___'+spec_dis_list[i][1] for i\\\n",
    "              in range(0,len(spec_dis_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new directory for resampled:\n",
    "! mkdir resampled\n",
    "! mkdir resampled/test\n",
    "! mkdir resampled/train\n",
    "! mkdir resampled/validation\n",
    "for folder in folders:\n",
    "    os.mkdir(os.path.join('resampled/test/',folder))\n",
    "    os.mkdir(os.path.join('resampled/train/',folder))\n",
    "    os.mkdir(os.path.join('resampled/validation/',folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a14d4",
   "metadata": {},
   "source": [
    "We now want to generate the train/val/test split for `resampled`, in order to get the randomized train validation and test directories we want. We will also use stratify by the folder name in order to keep the class balance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964b863",
   "metadata": {},
   "source": [
    "We will again add a `Folder` column to `resampled2` in order to stratify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder column\n",
    "spec_dis = [item.split(\"/\")[2] for item in resampled['ID']]\n",
    "spec_dis = pd.DataFrame({\"Folder\":spec_dis})\n",
    "\n",
    "# concatenate\n",
    "resampled = pd.concat([resampled, spec_dis], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adffeaa",
   "metadata": {},
   "source": [
    "Now we can do the train/validation/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e817f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_resampled, test_resampled = train_test_split(resampled, test_size=0.2, stratify = resampled['Folder'])\n",
    "train_r_resampled, val_resampled = train_test_split(train_resampled, test_size=0.3, stratify = train_resampled['Folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09268d10",
   "metadata": {},
   "source": [
    "Now we fill the `resampled` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2629f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IDs = train_r_resampled['ID'].to_list()\n",
    "test_IDs = test_resampled['ID'].to_list()\n",
    "val_IDs = val_resampled['ID'].to_list()\n",
    "\n",
    "# fill up resampled directory, taking into account train/test split:\n",
    "for ID in train_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled/train/'+folder)\n",
    "            \n",
    "for ID in test_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled/test/'+folder)\n",
    "      \n",
    "\n",
    "for ID in val_IDs:\n",
    "    for folder in folders:\n",
    "        if folder in ID:\n",
    "            safe_copy(ID, 'resampled/validation/'+folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a972a68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_r_resampled['Folder'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c51b1a",
   "metadata": {},
   "source": [
    "## CNN on `resampled2` data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd976b",
   "metadata": {},
   "source": [
    "Now that we've created the `resampled2` directory for easy loading in of the data using `ImageDataGenerator` function from the Keras library, we can begin to model a CNN for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12131b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27241d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the dimensions we want our images to be preprocessed to\n",
    "# (This will allow us to images of different resolutions)\n",
    "height = 256\n",
    "width = 256\n",
    "channels = 3\n",
    "\n",
    "# Create training image data generator.\n",
    "# We include data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range = 30,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Create validation image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create test image data generator.\n",
    "# Only apply rescaling to our validation data.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Finaly we specify where the images should be loaded from\n",
    "# as well as some additional attributes:\n",
    "train_generator=train_datagen.flow_from_directory('resampled2/train',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "validation_generator=validation_datagen.flow_from_directory('resampled2/validation',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory('resampled2/test',\n",
    "                                                 target_size=(height,width),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=3600,\n",
    "                                                 class_mode='binary')\n",
    "\n",
    "X_test, y_test = test_generator.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9763b1",
   "metadata": {},
   "source": [
    "We can look at some of the healthy/diseased images from our validation_generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c569c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of images from our validation generator: \n",
    "x, y = validation_generator.next() \n",
    "\n",
    "def label_names(labels, i):\n",
    "    if labels[i] == 1:\n",
    "        return 'Healthy'\n",
    "    elif labels[i] == 0:\n",
    "        return \"Diseased\"\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(9):\n",
    "    ax[i].imshow(x[i])\n",
    "    \n",
    "    # Set the title of the subplot\n",
    "    ax[i].set_title(label_names(y, i))\n",
    "    \n",
    "    # Hide the x and y ticks\n",
    "    ax[i].set_xticks([]) \n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "\n",
    "fig.suptitle(\"Diseased vs. Healthy Classification Labels\", size = 20)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83104050",
   "metadata": {},
   "source": [
    "We load in the Keras libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific neural network models & layer types\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d12bf",
   "metadata": {},
   "source": [
    "In order to model if the leaves are diseased/healthy, we will choose a simple CNN architecture at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe780cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_model = Sequential()\n",
    "\n",
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting\n",
    "CNN_model.add(Conv2D(4, kernel_size=(8,8), activation = 'relu', input_shape = (256, 256, 3)))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Conv2D(8, (8,8), activation='relu'))\n",
    "CNN_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the output of our convolutional layers\n",
    "CNN_model.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "CNN_model.add(Dense(128, activation='relu'))\n",
    "CNN_model.add(Dense(64, activation='relu'))\n",
    "CNN_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Print out a summary of the network\n",
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb35d1d",
   "metadata": {},
   "source": [
    "We see that the model inclues around 3.4 million parameters. We will now compile and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the desired loss function, optimizer, and metric(s) to track\n",
    "CNN_model.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a4d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model on the training data, with the validation data\n",
    "CNN_model.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    verbose = 1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114c97f",
   "metadata": {},
   "source": [
    "With our simple CNN, we achieved an accuracy of 87.99% with a validation accuracy of 90%! Not bad at all! Now we need to see how it does on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473f9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = CNN_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbcf8e5",
   "metadata": {},
   "source": [
    "Test accuracy of 89.78%! So we see that even with a relatively simple CNN, we are still able to have a pretty good accuracy score, AND without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceaa1ec",
   "metadata": {},
   "source": [
    "### CNN confusion matrix for the modelling of `resampled2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad7c65",
   "metadata": {},
   "source": [
    "Now we can look at the confusion matrix to see where we went wrong. But first we convert the soft predictions of the model into hard predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the soft predictions into hard predictions\n",
    "probs = CNN_model.predict(X_test).flatten()\n",
    "predict = np.empty(shape = (3600,))\n",
    "for i in range(0,len(probs)):\n",
    "    if probs[i] >= .5:\n",
    "        predict[i] = 1\n",
    "    else:\n",
    "        predict[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbc4af",
   "metadata": {},
   "source": [
    "Now we can create the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the predicted labels for each test image.\n",
    "y_predict = predict\n",
    "\n",
    "# Create the confusion matrix using sklearn \n",
    "conf_mat = confusion_matrix(y_test, y_predict)\n",
    "normalized_conf_mat = conf_mat / conf_mat.sum(axis=1)\n",
    "\n",
    "plt.figure(figsize = (9,7))\n",
    "sns.heatmap(normalized_conf_mat,\n",
    "            annot=True,\n",
    "            cbar=False,\n",
    "            cmap=\"rocket_r\",\n",
    "            linewidths=1\n",
    "           )\n",
    "plt.title('Confusion Matrix',size = 25,y=1.01)\n",
    "plt.xlabel(\"Predicted Label\", size = 20)\n",
    "plt.xticks(ticks = [0.5,1.5], labels = ['Diseased', 'Healthy'], ha = 'center')\n",
    "plt.yticks(ticks = [0.5,1.5], labels = ['Diseased', 'Healthy'], ha = 'center')\n",
    "plt.ylabel(\"True Label\", size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbe91e",
   "metadata": {},
   "source": [
    "We see from the above confusion matrix that we much more often incorrectly predict healthy  leaves when they are in fact diseased (16%), as compared to predicting diseased leaves when in fact they are healthy (4.4%). So we tend to overpredict that leaves are healthy (false negative). We can also look at the classifation report, noting that here 0 is diseased, and 1 is healthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71736c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_initial = classification_report(y_test, y_predict)\n",
    "print(report_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc10ba2",
   "metadata": {},
   "source": [
    "We see from the above that the recall (= TP/(TP+FN)) here is lower (84%) for the diseased as opposed to for the healthy (96%), meaning more of the diseased are incorrectly getting labelled as healthy, which makes sense, as sometimes the diseased leaves only subtely look diseased. Correspondingly, we note that the precision is lower for the healthy (86%) than for the diseased (96%), meaning the labelling of 'healthy' is less precise. From this, we can see that the `Healthy` label is use more than the `Diseased` label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"Diseased\", \"Healthy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40454cbf",
   "metadata": {},
   "source": [
    "We can look at the incorrectly labelled leaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c772b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "incorrect_photos = y_test != y_predict\n",
    "\n",
    "num_images = np.count_nonzero(incorrect_photos)\n",
    "columns = 3\n",
    "rows = math.ceil(num_images / columns)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=columns, figsize=(10, 3.5*rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, image in enumerate(X_test[incorrect_photos]):\n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_label = label_names[int(y_predict[incorrect_photos][i])]\n",
    "    \n",
    "    # Get actual label\n",
    "    true_label =  label_names[int(y_test[incorrect_photos][i])]\n",
    "    \n",
    "    # Set the title of the subplot\n",
    "    axes[i].set_title(f\"Predicted: {predicted_label}\\n True: {true_label}\")\n",
    "    \n",
    "    # Hide the x and y ticks to make \n",
    "    axes[i].set_xticks([]) \n",
    "    axes[i].set_yticks([])\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_images, rows*columns):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931619ba",
   "metadata": {},
   "source": [
    "As expected, just skimming through the images we see that many more are labelled `Healthy` incorrectly than `Diseased`, which is consistent with our confusion matrix and with the precision/recall analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626ccac",
   "metadata": {},
   "source": [
    "### 2nd attempt at CNN modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816dae3",
   "metadata": {},
   "source": [
    "We want to be able to analyze the images more subtlely, so we will try adding more convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3131228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CNN_model_2nd = Sequential()\n",
    "\n",
    "# Create simple CNN model architecture with Pooling for dimensionality reduction \n",
    "# and Dropout to reduce overfitting. go from 4 convolutions -> 8 in the first layer\n",
    "CNN_model_2nd.add(Conv2D(8, kernel_size=(8, 8), activation = 'relu', input_shape = (256, 256, 3)))\n",
    "CNN_model_2nd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model_2nd.add(Dropout(0.25))\n",
    "\n",
    "# go from 8 convolutions -> 16 in the first layer\n",
    "CNN_model_2nd.add(Conv2D(16, (8, 8), activation='relu'))\n",
    "CNN_model_2nd.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "CNN_model_2nd.add(Dropout(0.25))\n",
    "\n",
    "# Flatten the output of our convolutional layers\n",
    "CNN_model_2nd.add(Flatten())\n",
    "\n",
    "# Add dense layers\n",
    "CNN_model_2nd.add(Dense(128, activation='relu'))\n",
    "CNN_model_2nd.add(Dense(64, activation='relu'))\n",
    "CNN_model_2nd.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Print out a summary of the network\n",
    "CNN_model_2nd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the desired loss function, optimizer, and metric(s) to track\n",
    "CNN_model_2nd.compile(loss = 'binary_crossentropy',\n",
    "                  optimizer = 'Adam',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f89148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model on the training data, with the validation data\n",
    "CNN_model_2nd.fit(train_generator,\n",
    "                    epochs=5,\n",
    "                    verbose = 1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9aad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = CNN_model_2nd.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72fb01a",
   "metadata": {},
   "source": [
    "Oddly enough, adding too many convolutions seems to make the system very inaccurate, basically as good as chance (52% for the test accuracy), with massive overfitting (83.93% accuracy during training). One possibility for this is that with more added complexity to the CNN, it simply needs more epochs to fully train the network for it to be accurate. So we will be satisfied for now with our first try, which gave pretty good accuracy with a simple format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3166e48",
   "metadata": {},
   "source": [
    "### Random sampling of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98925a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can get the images for a given class and have the associated disease label:\n",
    "\n",
    "images = []\n",
    "disease_label = []\n",
    "disease_name = []\n",
    "plant_name = []\n",
    "for i in range(0,len(filepaths_fulldf_unique)):\n",
    "    for f in glob.iglob(filepaths_fulldf_unique[i]):\n",
    "        # convert to grayscale to save space\n",
    "        # images.append(np.asarray(Image.open(f).convert('L')))\n",
    "        images.append(np.asarray(Image.open(f)))\n",
    "        disease_name.append(filepaths_fulldf_unique[i].split(\"___\")[1].split(\"/\")[0])\n",
    "        plant_name.append(filepaths_fulldf_unique[i].split(\"/\")[2].split(\"___\")[0])\n",
    "        \n",
    "        if \"healthy\" in filepaths_fulldf_unique[i]:\n",
    "            disease_label.append(0)\n",
    "        else:\n",
    "            disease_label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4563c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled2[['Species','Disease_Type']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d29daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 10 images randomly\n",
    "import random as rand\n",
    "rand10 = rand.sample(range(len(images)),10)\n",
    "random_sampling_images = np.array([images[i] for i in rand10])\n",
    "random_sampling_labels = np.array([disease_name[i] for i in rand10])\n",
    "random_sampling_plant = np.array([plant_name[i] for i in rand10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize = (10,10))\n",
    "for i, ax in zip(range(9), axes.ravel()):\n",
    "    ax.imshow(random_sampling_images[i])\n",
    "    ax.axes.xaxis.set_visible(False)\n",
    "    ax.axes.yaxis.set_visible(False)\n",
    "    ax.set_title(f\"{random_sampling_plant[i]}, {random_sampling_labels[i]}\")\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
